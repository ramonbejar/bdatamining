{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><span style=\"color:blue; font-family:Georgia;  font-size:2em;\"><h1>Mining Frequent Itemsets (1)</h1></span></center>\n",
    "    <p> </p>\n",
    "    <p> </p>\n",
    "    <center><span style=\"color:blue; font-family:Georgia;  font-size:1em;\">\n",
    "    Ramon BÃ©jar</span></center>\n",
    "    <canvas id=\"myCanvas\" width=\"200\" height=\"100\" style=\"border:0px solid\"></canvas>\n",
    "    <center>Data mining - Master on Computer Science</center>\n",
    "    <center><img src=\"M-UdL2.png\"  width=\"200\" alt=\"UdL Logo\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this notebook, we consider the problem of mining frequent itemsets when the input is a collection of transactions, where a transaction is again an itemset. The frequent itemsets we want to mine from the transactions will be subsets of these transactions, and so we are interested in discovering the most frequent subsets in such transactions. We are going to present two widely used algorithms: the *A-Priori algorithm* and the *FP-Growth algorithm*. The last one is implemented in the machine learning library of spark. In this notebook we talk about the problem, the A-Priori Algorithm and association rules. The second algorithm is presented in a next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Preliminary start-up code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark-3.0.0-bin-hadoop2.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.117:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\\n\",\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print ( spark_home )\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mining Frequent Itemsets\n",
    "\n",
    "Consider a sequence/set of **transactions**:\n",
    "\n",
    " $ T = \\{ T_1,T_2,\\ldots,T_m \\} $\n",
    " \n",
    "where each $T_i$ is a set of items that come from some catalog of possible items $I$. That is, $\\forall T_i, T_i \\subseteq I$.\n",
    "\n",
    "Given a support threshold $\\theta \\in [0,1]$, we say that a itemset $P \\subseteq I$ is $\\theta-$frequent if its support among  $T$ is $ \\geq \\theta  $:\n",
    "\n",
    "$$ \\frac{|\\{T_i  | P \\subseteq T_i, T_i \\in T \\}|}{|T|}  \\geq \\theta  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Marketing in Retail Stores\n",
    "\n",
    "The context where this problem has been defined and studied extensively is the analysis of customer habits in traditional retail stores. \n",
    "\n",
    "Discovering frequently bought together products (items) in different purchases (transactions) can be useful for making marketing campaigns directed towards capturing the interest of most customers of the store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, suppose a supermarket finds that the pair of products:\n",
    "\n",
    "$$ \\{ beer, cocacola \\} $$\n",
    "\n",
    "is $0.76-$frequent among the transactions of the last month and that the pair of products:\n",
    "\n",
    "$$ \\{ beer, nachos \\} $$\n",
    "\n",
    "is $0.60-$frequent. Then, we may think that it may be good developing marketing strategies that promote buying such pairs of products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some straightforward marketing options:\n",
    "- Put these products close to each other in some location of the store $\\Rightarrow$ to remember to ALL customers that it may be good to buy them together\n",
    "- Offer discounts if you buy them together $\\Rightarrow$ To offer an small incentive towards buying them, in case customers think they do not need them together \"today\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What about on-line stores?\n",
    "\n",
    "Observe that for the case of on-line shops, focusing only on the most frequent subsets may be of less relevance, as in a on-line store one usually is interested in making recommendations personalized for every costumer (as there is no real limitation of a single selling space shared by all the costumers). In other words, we can think about presenting a totally personalized shop for every costumer of an on-line shop.\n",
    "\n",
    "This approach will be considered when we talk about recommender systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The A-Priori Algorithm\n",
    "\n",
    "This algorithm is based on exploiting the following basic principle, the monotonicity of itemsets:\n",
    "\n",
    "    If a set P of items is frequent, then so is every subset of P\n",
    "            \n",
    "More concretely, if $P$ is $\\theta-$frequent, then every subset of $P$ will be *at least* $\\theta$-frequent. The typical situation will be that for a fixed frequency $ \\theta $, there will be much less frequent sets of size $k$ than of size $k-1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the particular domain of retail stores, the typical transaction size will be small, so when considering high frequency thresholds, we will not find many frequent item sets of big size.\n",
    "\n",
    "So, the A-Priori algorithm is based on first finding smaller frequent-items sets, and then going to the next size but considering only sets such that any of their subsets has been found previously to be frequent. This suggests an iterative algorithm that first looks for frequent itemsets of size 1, then of size 2, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## High-level pseudo-code of the A-Priori algorithm\n",
    "\n",
    ">$L_1$ := Find frequent elements (T,$\\theta$)  \n",
    ">k=2  \n",
    ">While ($L_{k-1}$ is not empty) do:  \n",
    ">>$C_k = \\{ P \\ | \\ |P|=k, \\forall S_j \\subseteq P, |S_j|=k\\!-\\!1 \\rightarrow S_j \\in L_{k-1}\\}$  \n",
    ">>$L_k = \\{ P \\ | \\ P \\in C_k, support(P,T) \\geq \\theta \\}$  \n",
    ">>k=k+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A-Priori Implementation in map-reduce\n",
    "\n",
    "To implement the algorithm with map-reduce, in principle we could perform in every iteration two main tasks:\n",
    "\n",
    "1. Compute $C_k$ from $L_{k-1}$ : This can be done as a series of map transformations from $ L_{k-1} $ and $I$ \n",
    "2. Compute $L_k$ from $C_k$ and $T$ : This can be done as a series of map transformations from $C_{k}$ and $T$ and a final filter transformation that uses the $\\theta$ value\n",
    "\n",
    "However, it is more efficient to avoid the explicit computation of the whole set $C_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, to compute for every transaction $T_i$, those subsets $P \\subseteq T_i$ that satisfy the **condition** of belonging to $C_k$:\n",
    "\n",
    ">$f_k(t \\in T, L_{k-1})$: \n",
    ">$ \\ \\ \\ \\ l = [ (P_i,1) \\ | \\ P_i \\in {t \\choose k}, \\forall S_j \\subset P_i, |S_j|=k\\!-\\!1 \\rightarrow S_j \\in L_{k-1}]$  \n",
    ">$ \\ \\ \\ \\ \\mbox{return} \\ l$\n",
    "\n",
    "Then perform $T.flatMap( \\lambda \\ t : f_k(t,L_{k-1}) )$ to get the elements from $C_k$ that belong to **at least one** $t \\in T$ \n",
    "\n",
    "So, in a next reduceByKey operation, we can count in how many transactions appears each such $P_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observe that for the Spark framework, the previous function cannot be directly applied in a simple map transformation from an RDD with $T$ if the set $L_{k-1}$ **is also an RDD**. Options:\n",
    "\n",
    "1. Transform the RDD of $L_{k-1}$ to a python list.\n",
    "2. Transform the RDD of $L_{k-1}$ to a spark broadcast variable.\n",
    "\n",
    "Even if the second option is more efficient, both solutions can only be applied if $L_{k-1}$  fits into the memory of any computation node.\n",
    "\n",
    "$\\Rightarrow$ we may have scaling issues if $L_{k-1}$ grows too much\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other options:\n",
    "\n",
    "3. Store $L_{k-1}$ in a database that can be accessed by any node of the cluster\n",
    "4. Perform a join between $P_k=\\{ (t,P_i) \\ | \\ t \\in T, P_i \\in {t \\choose k} \\}$ and  $L_{k-1}$ where the condition for the join of $(t,P_i)$ with $S_j \\in L_{k-1}$ is that: \n",
    "$$ S_j \\subset P_i $$\n",
    "Then, observe that for any $(t,P_i)$ such that all its $k\\!-\\!1$-subsets are in $L_{k-1}$, the final join will contain $k$ copies of $(t,P_i)$ (each one with a different subset $S_j$).\n",
    "\n",
    "This kind of conditional join can be done in spark when working with spark dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A simple implementation for k up to 2\n",
    "\n",
    "Assume $L_1$ is much smaller than $T$ and so it can fit into the memory of any node. Then, we have a very simple implementation for the case of $k=2$:\n",
    "1. Use a map-reduceByKey-filter chain to compute $L_1$ from $T$, and to compute a version of $T$ with only items present in $L_1$ (call it $T_{L_1}$) \n",
    "2. Use a map to compute candidate pairs for each $t \\in T$ from $T_{L_1}$ $(C_2(T))$\n",
    "3. Use a reduceByKey-filter chain to compute $L_2$ from $C_2(T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 1: Compute $L_1$ and $T_{L_1}$\n",
    "\n",
    "For each t in rddT, this function maps $t=[it_1,\\ldots,it_k]$ to $[(it_1,1),\\ldots,(it_k,1)]$, and then with reduceByKey and filter detects those items that are $\\theta-$frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the rdd with frequent singleton sets (L_1)\n",
    "def computeL1 ( rddT, numtrans, theta ):\n",
    "  rddtemp = rddT.flatMap( lambda t : [ (it,1) for it in t ] ).reduceByKey( lambda a,b : a+b  )\n",
    "  return rddtemp.filter( lambda x : (float(x[1])/numtrans) >= theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, once we convert L1 to a python list back to the driver program, we compute the RDD of $T_{L_1}$ from $T$ and  L1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Map any transaction to its version without elements not in L1\n",
    "# L1 must be a python list, not a RDD\n",
    "def computeTfilteredByL1( seqOfT, L1 ):\n",
    "    for t in seqOfT:\n",
    "       yield [ it for it in t if (it in L1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe that we expect to call this function with mapPartitions, not map, so the first argument is a sequence of transactions, provided by a python iterable object, and the result will be also a python iterable object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 2: Compute $C_2(T)$ from $T_{L_1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# For each t in seqofFilteredT (they come from T_{L_1}), compute pairs (a,b) from t that belong to C_2\n",
    "def generateC2( seqofFilteredT ):\n",
    "    for t in seqofFilteredT:\n",
    "      cpairslist = []\n",
    "      for (a,b) in [ (a,b) for i,a in enumerate(t[:-1]) for b in t[i+1:] ]:\n",
    "                cpairslist.append( ((a,b),1) if (a <= b) else ((b,a),1)  )         \n",
    "      yield cpairslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we apply flatMap to the resulting RDD of this function, each element $((a,b),1)$ will appear as many times as the number of transactions where $(a,b)$ appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 3: Compute $L_2$ from $C_2(T)$\n",
    "Here we assume that rddC2T is the *flattened* version of the RDD obtained with generateC2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def computeL2( rddC2T, numtrans, theta ):\n",
    "    pairsCountedrdd = rddC2T.reduceByKey( lambda v1,v2 : v1+v2 )\n",
    "    # Finally, filter out from the previous rdd those pairs with frequency below theta\n",
    "    return pairsCountedrdd.filter( lambda x : (float(x[1])/numtrans) >= theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What we do is first to count the number of transactions with a same pair $(a,b)$, and then we filter those pairs that are $\\theta-$ frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A-Priori Execution Example\n",
    "To illustrate the execution of the A-Priori algorithm, consider the following set of transactions and $\\theta=0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beeranddiapers1 = [['beer','diapers','cheese'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','pizza','yogurt'],\n",
    "                   ['beer','diapers','milk'],\n",
    "                   ['beer','diapers','milk','pizza'],\n",
    "                   ['beer','diapers','toothpaste'],\n",
    "                   ['beer','diapers','icecream'],\n",
    "                   ['beer','diapers','pizza','yogurt']]\n",
    "numtrans = len(beeranddiapers1)\n",
    "theta=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rddT = sc.parallelize(beeranddiapers1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Phase 1**: compute the RDD for $L_1$ and $T_{L_1}$ and convert it to a python list back to the driver (remember, this may not scale well depending on the size of $L_1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rddL1 = computeL1 ( rddT, numtrans, theta )\n",
    "L1 = rddL1.keys().collect() # we need only the items (keys) in final L1\n",
    "TL1 = rddT.mapPartitions( lambda seqOfT : computeTfilteredByL1( seqOfT, L1 )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " L1 items:  ['pizza', 'milk', 'diapers', 'yogurt', 'beer']\n",
      " Transactions with only frequent elements:  [['beer', 'diapers'], ['beer', 'diapers', 'pizza'], ['beer', 'diapers', 'pizza', 'yogurt'], ['beer', 'diapers', 'milk'], ['beer', 'diapers', 'milk', 'pizza'], ['beer', 'diapers'], ['beer', 'diapers'], ['beer', 'diapers', 'pizza', 'yogurt']]\n"
     ]
    }
   ],
   "source": [
    "print(\" L1 items: \", L1)\n",
    "print(\" Transactions with only frequent elements: \", TL1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Phase 2**: Compute $C_2(T)$ from $T_{L_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rddC2T = TL1.mapPartitions( lambda seqOfFilteredT : generateC2( seqOfFilteredT ) )\n",
    "rddC2TFlat = rddC2T.flatMap( lambda x : x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened C2T:  [(('beer', 'diapers'), 1), (('beer', 'diapers'), 1), (('beer', 'pizza'), 1), (('diapers', 'pizza'), 1), (('beer', 'diapers'), 1), (('beer', 'pizza'), 1), (('beer', 'yogurt'), 1), (('diapers', 'pizza'), 1), (('diapers', 'yogurt'), 1), (('pizza', 'yogurt'), 1), (('beer', 'diapers'), 1), (('beer', 'milk'), 1), (('diapers', 'milk'), 1), (('beer', 'diapers'), 1), (('beer', 'milk'), 1), (('beer', 'pizza'), 1), (('diapers', 'milk'), 1), (('diapers', 'pizza'), 1), (('milk', 'pizza'), 1), (('beer', 'diapers'), 1), (('beer', 'diapers'), 1), (('beer', 'diapers'), 1), (('beer', 'pizza'), 1), (('beer', 'yogurt'), 1), (('diapers', 'pizza'), 1), (('diapers', 'yogurt'), 1), (('pizza', 'yogurt'), 1)]\n"
     ]
    }
   ],
   "source": [
    "print( \"flattened C2T: \", rddC2TFlat.collect() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Phase 3**: Compute ð¿2 from ð¶2(ð‘‡)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rddL2 = computeL2( rddC2TFlat, numtrans, theta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('beer', 'diapers'), 8)\n",
      "(('beer', 'pizza'), 4)\n",
      "(('diapers', 'pizza'), 4)\n",
      "(('diapers', 'yogurt'), 2)\n",
      "(('pizza', 'yogurt'), 2)\n",
      "(('beer', 'milk'), 2)\n",
      "(('beer', 'yogurt'), 2)\n",
      "(('diapers', 'milk'), 2)\n"
     ]
    }
   ],
   "source": [
    "rddL2 = rddL2.sortBy(lambda a: -a[1])\n",
    "for it in rddL2.toLocalIterator():\n",
    "    print (it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you can see, beer and diapers make the most frequent pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding Association Rules\n",
    "\n",
    "The original application domain of the frequent itemsets problem was finding good marketing strategies in traditional retail stores. For example, in a famous application of this problem, it was found that\n",
    "most people that bought diappers were also buying beer. This suggests a possible association rule:\n",
    "$$ diapers \\rightarrow beer $$\n",
    "\n",
    "To quantify how strong/realistic this association rule can be considered, we may compute two measures for such rule from our dataset, using the information computed by the frequent itemsets algorithm, the **confidence**  and the **interest**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confidence\n",
    "\n",
    "The first such measure is the **confidence** for the rule. The confidence for the rule $ diapers \\rightarrow beer $ is the ratio:\n",
    "$$ \\frac{support( {diapers,beer} )}{support( {diapers})} $$\n",
    "where support(set) is the number of transactions where the set is found.\n",
    "\n",
    "Observe that this quantity will be high when the fraction of transactions with diapers that also contain beer is high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interest \n",
    "\n",
    "But observe that this alone does not mean that the rule {diapers} -> {beer} necessarily shows a *true relationship*. That is, it could happen that all the costumers buy beer, so the  rule {diapers} -> {beer} does not really provide an useful information to discover when people buys **more beer**. To quantify this situation, we need also to quantify the **interest** of the rule {I} -> {j} as the difference between its confidence and the frequency of {j}:\n",
    "\n",
    "$$ confidence ( {diapers} \\rightarrow {beer} ) - frequency( {beer} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe that the interest can be positive, zero o negative:\n",
    "\n",
    "- A negative interest indicates a negative effect, i.e. that when {I} is present {j} is less likely to be present than in general.\n",
    "- A positive interest indicates a positive effect, i.e. that when {I} is present {j} is more likely to be present than in general.\n",
    "- A zero interest indicates no significative effect of {I} to {j}: {I} being present does not affect the frequency of {j}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the diapers-beer example mentioned, the interest was positive, meaning that when diapers was present the relative frequency of beer was higher than when looking for beer in all the transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, towards building a marketing recommendation system, it can be interesting to use such association rules, and select the most interesting ones. For example, given a set of frequent itemsets P, we can compute the confidence and interest of any association rule of the kind $P\\setminus {j} \\rightarrow {j}$, for each item j present in P.\n",
    "\n",
    "For a given frequent itemset P, observe that we can build $|P|$ different association rules of such kind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To consider the difference between the confidence and the interest of an association rule consider the following transaction set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beeranddiapers1 = [['beer','diapers','cheese'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','milk'],\n",
    "                   ['beer','diapers','milk','pizza'],\n",
    "                   ['beer','diapers','toothpaste'],\n",
    "                   ['beer','diapers','icecream'],\n",
    "                   ['beer','diapers','pizza','yogurt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have that the confidence for the rule: \n",
    "$$ {diapers} -> {beer} $$\n",
    "\n",
    "  is:\n",
    "\n",
    "$$ \\frac{support(\\{beer,diapers\\})}{support(\\{diapers\\})} = \\frac{8}{8} = 1 $$ \n",
    "\n",
    "So, the confidence of the rule is maximum. However, we cannot strongly suggest that diapers make more likely to\n",
    "have beer, because the frequency of beer is as high as the frequency of beer when diapers are present:\n",
    "\n",
    "$$ confidence ( \\{diapers\\} \\rightarrow \\{beer\\} ) - frequency( \\{beer\\} ) = 1 - 1 = 0$$\n",
    "\n",
    "That is, the interest of the rule for this transaction set is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider this second example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beeranddiapers2 = [['beer','diapers','cheese'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','milk'],\n",
    "                   ['yogurt','milk','pizza'],\n",
    "                   ['pizza','toothpaste'],\n",
    "                   ['yogurt','icecream'],\n",
    "                   ['milk','icecream'],\n",
    "                   ['pizza','icecream'],\n",
    "                   ['cheese','icecream'],\n",
    "                   ['beer','pizza','yogurt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This time, we have that the confidence for the rule: \n",
    "$$ {diapers} -> {beer} $$\n",
    "is:\n",
    "\n",
    "$$ \\frac{support(\\{beer,diapers\\})}{support(\\{diapers\\})} = \\frac{4}{4} = 1 $$\n",
    "\n",
    "So, again the confidence of the rule is maximum. However, in this case the rule is really signalling a stronger relationship between both products, because the interest is:\n",
    "\n",
    "\n",
    "$$ confidence ( \\{diapers\\} \\rightarrow \\{beer\\} ) - freq( \\{beer\\} ) = 1 - (5/11) = 0.54 $$\n",
    "\n",
    "That is, in the second transaction set the presence of diapers makes more likely the presence of beer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise (will be graded)\n",
    "\n",
    "Consider the following programming exercise. Given the information of the frequent singletons $(L_1)$ and frequent pairs $(L_2)$ we compute with our previous implementation of A-Priori for k=2, implement in spark functions to compute the  **confidence** and **interest** of all the binary rules we can build from the set $L_2$. As the dataset to test your code, **compute** a set of transactions from this data set:\n",
    "\n",
    "https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset\n",
    "\n",
    "Or this smaller one (that it already contains one transaction per line) if you are not able to work with the previous one:\n",
    "\n",
    "https://www.kaggle.com/shazadudwadia/supermarket?select=GroceryStoreDataSet.csv\n",
    "\n",
    "\n",
    "Try these values for $\\theta$: 0.01, 0.1, 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once you have computed the sets T, L1, $T_{L1}$ and L2, your program should follow these steps:\n",
    "\n",
    "1. Map, using mapPartitions, each frequent pair in the RDD with L2 to its list of binary association rules (two association rules per each different frequent pair). Use then the flattened version of this RDD.\n",
    "2. Map each association rule of the previous resulting RDD, to a triple with (rule,confidence,interest). Observe that you will need to use the information in L1 and the number of transactions to compute these values. You can use the version of L1 stored as a python list in the driver (so it can be passed inside functions passed to spark tasks).\n",
    "3. Finally, sort the association rules by their interest, and show back in the driver program the first 10 most interesting rules"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
