{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><span style=\"color:blue; font-family:Georgia;  font-size:2em;\">Mining Frequent Itemsets (2)</span></center>\n",
    "    <p> </p>\n",
    "    <p> </p>\n",
    "    <center><span style=\"color:blue; font-family:Georgia;  font-size:1em;\">\n",
    "    Ramon BÃ©jar</span></center>\n",
    "    <canvas id=\"myCanvas\" width=\"200\" height=\"100\" style=\"border:0px solid\"></canvas>\n",
    "    <center>Data mining - Master on Computer Science</center>\n",
    "    <center><img src=\"M-UdL2.png\"  width=\"200\" alt=\"UdL Logo\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Preliminary start-up code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark-3.0.0-bin-hadoop2.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.117:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\\n\",\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print ( spark_home )\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Mining Frequent Itemsets\n",
    "\n",
    "Consider a sequence/set of **transactions**:\n",
    "\n",
    " $ T = \\{ T_1,T_2,\\ldots,T_m \\} $\n",
    " \n",
    "where each $T_i$ is a set of items that come from some catalog of possible items $I$. That is, $\\forall T_i, T_i \\subseteq I$.\n",
    "\n",
    "Given a support threshold $\\theta \\in [0,1]$, we say that a itemset $P \\subseteq I$ is $\\theta-$frequent if its support among  $T$ is $ \\geq \\theta  $:\n",
    "\n",
    "$$ \\frac{|\\{T_i  | P \\subseteq T_i, T_i \\in T \\}|}{|T|}  \\geq \\theta  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The FP-Growth Algorithm\n",
    "\n",
    "We have discussed in the A-Priori frequent itemsets mining algorithm that a major bottleneck in the performance is the possible increasing number of candidate itemsets as the size $k$ considered increases in every stage of the algorithm.\n",
    "\n",
    "We present next an algorithm, the FP-Growth, that avoids this candidate set generation of itemsets of size k ($C_k$) to be able to generate the actual set of frequent itemsets of size k ($L_k$). The algorithm achieves this improvement thanks to a data structure called a **prefix-search tree**. This tree is a *compact* representation of the set of transactions, but in such a way that allows a traversal of its branches to efficiently find the frequent itemsets of any size k scanning the tree only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building a prefix-search-tree with FP-Growth\n",
    "\n",
    "Consider the following set of transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "T = [['f','c','a','m','p','i'],\n",
    "     ['f','b','a','c','m','o'],\n",
    "     ['b','f','j'],\n",
    "     ['c','b','p','s'],\n",
    "     ['a','c','f','m','p','l']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where the corresponding set $L_1$ with $\\theta=3/5$ is  $ \\{ f:4, c:4, a:3, b:3, m:3, p:3 \\} $. The prefix-search tree for $T$ represents in a compact way the five transactions, such that then we can extract any frequent subset with a recursive traversal of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, as we did with the A-Priori algorithm, we eliminate from every transaction elements not in $L_1$, but we also sort the remaining elements by descending number of ocurrences in $T$ ($L_1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "TfilAndSorted = [['f','c','a','m','p'],\n",
    "                 ['f','c','a','b','m'],\n",
    "                 ['f','b'],\n",
    "                 ['c','b','p'],\n",
    "                 ['f','c','a','m','p']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First step: insert $\\color{red}{\\{f,c,a,m,p\\}}$\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"exampleFPtree-s1.png\" width =\"90px\"  />\n",
    "<div style=\"width:860px; float: left;\">\n",
    "<ul>\n",
    "<li>Starting from a tree with only the root node, we insert each element of $\\color{red}{\\{f,c,a,m,p\\}}$ respecting the order of the elements.\n",
    "<li>The number that labels each node represents the number of processed transactions of the DB that contain the subset given by the path of items from the root to the node.\n",
    "So there is:\n",
    "    <p>\n",
    "    <ul>\n",
    "     <li> one subset $\\{f\\}$\n",
    "     <li> one subset $\\{f,c\\}$\n",
    "     <li> one subset $\\{f,c,a\\}$\n",
    "     <li> one subset $\\{f,c,a,m\\}$\n",
    "     <li> one subset $\\{f,c,a,m,p\\}$\n",
    "    </ul>\n",
    "    </p>\n",
    "</ul>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Second step: insert  $\\color{red}{\\{f,c,a,b,m\\}}$\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"exampleFPtree-s2.png\" height =\"120px\"  />\n",
    "<div style=\"width:860px; float: left;\">\n",
    "For the second transaction, the first three elements $\\color{red}{(f,c,a)}$ are found in the same order starting from the root, so we simply increment their counters\n",
    "    \n",
    "But when we arrive to the element $\\color{red}{b}$, we need to make a new sub-branch (from the element $\\color{red}{a}$) with initial counters for $\\color{red}{b}$ and $\\color{red}{m}$ equal to 1\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Third step: insert  $\\color{red}{\\{f,b\\}}$\n",
    "\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"exampleFPtree-s3.png\" height=\"125px\" />\n",
    "<div style=\"width:860px; float: left;\">\n",
    "For the third transation, we increment the counter for $\\color{red}{f}$\n",
    "And make a new sub-branch from $\\color{red}{f}$ for $\\color{red}{b}$, with initial counter 1\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fourth step: insert $\\color{red}{\\{c,b,p\\}}$\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"exampleFPtree-s4.png\" heigth=\"125px\" />\n",
    "<div style=\"width:860px; float: left;\">\n",
    "The next transaction, starts with an element, $\\color{red}{c}$, that is not $f$, so we make a new branch from the root of the tree with initial counters equal to 1.    \n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fith step: insert  $\\color{red}{\\{f,c,a,m,p\\}}$\n",
    "\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"exampleFPtree-s5.png\" height=\"125px\" />\n",
    "<div style=\"width:860px; float: left;\">\n",
    "Finally, when we insert the last transaction, $\\color{red}{\\{ f,c,a,m,p \\}}$, because it is already represented in the first branch of the tree, processing their elements by order simply causes to increment the counters of that branch\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, the prefix-search tree represents in a compact way:\n",
    "- the four transactions that contain the most frequent element (f), with the subtree with root f. Observe that this subtree contains three subbranches (that share some nodes), where the branch f-c-a-m-p encodes the information of the first and last transactions of the DB, the branch f-c-a-b-m encodes the second transaction, and the branch f-b the third transaction.\n",
    "- Finally, the branch c-b-p represents the fourth transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a DB of transactions and the ordered set L1 computed with a particular minSupport, the following algorithm (pseudo-code) can be used to build such prefix-search tree\n",
    "\n",
    "```python\n",
    " def buildFPtree( L1, DB ):\n",
    "   # create initial tree with only a root\n",
    "   fptree = (root,())\n",
    "   L1 = sortByFrequency(L1)\n",
    "   for tran in DB:\n",
    "      sortedtran = sortByFrequency(tran,L1)\n",
    "      insertTree(sortedtran,fptree)\n",
    "   return fptree\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ```python\n",
    " def insertTree( sortedtran, fptree ):\n",
    "   firstit = head(sortedtran)\n",
    "   if (fptree has child node with name firstit):\n",
    "       fpsubtree = getsubtree( fptree, firstit ) \n",
    "       incrementRootCounter( fpsubtree )\n",
    "   else:\n",
    "      fpsubtree = ((firstit,1),())  \n",
    "      addchild(fptree,fpsubtree)    \n",
    "   tailtran = sortedtran.delete(firstit)\n",
    "   if (tailtran not empty) insertTree( tailtran, fpsubtree )   \n",
    "```\n",
    "\n",
    "Observe that the overall process for building the prefix-search tree needs two scans of the transaction DB, one for building the ordered set $L_1$, and a second one for building the tree from $L_1$ and the DB of transactions, where each transaction is processed once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Once we have such tree, it is possible to compute the **full set of frequent subsets** with the given minSupport, from our DB of transactions with a recursive algorithm that traverses the tree recursively. The details of such algorithm (and of the previous prefix-search tree construction) can be found in the following paper:\n",
    "\n",
    "\n",
    ">  Jiawei Han, Jian Pei, Yiwen Yin, Runying Mao. *Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach*. Data Mining Knowledge Discovery. 8(1): 53-87 (2004).\n",
    "> Link: http://hanj.cs.illinois.edu/pdf/dami04_fptree.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A distributed FPgrowth\n",
    "\n",
    "Spark includes a distributed version of FPgrowth, that works by creating different independent prefix-search trees in different partitions. The tree of each partition is used to count different patterns (**ordered subsets**), such that a given pattern will be responsability of the tree of one specific partition. This algorithm is described in the paper:\n",
    "\n",
    ">  Haoyuan Li, Yi Wang, Dong Zhang, Ming Zhang, Edward Y. Chang. *PFP: parallel FP-growth for query recommendation*. RecSys 2008: 107-114.\n",
    "> Link: http://dl.acm.org/citation.cfm?doid=1454008.1454027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "They key of the algorithm is how it divides the possible patterns to count in the different prefix-search trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dividing patterns by element groups\n",
    "\n",
    "Assume we order elements by some given fixed order (in our case by their position in $L_1$). Then, if we divide the elements of $L_1$ in $m$ different groups, the distributed version of FP-growth assigns a pattern of ordered elements:\n",
    "$$ [e_1,e_2,\\ldots,e_k] $$\n",
    "to the group where $e_k$ (the last element) belongs. So, any pattern will be **uniquely assigned to one group**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, given a transaction that may have elements from different groups: $t=[t_1,t_2,\\ldots,t_h ]$, the algorithm creates different **group-dependent transactions** from $t$ as follows:\n",
    " ```python\n",
    "def makeGroupDependentTransactions(t,groupId):\n",
    "    groupOft = dict()\n",
    "    for i = h-1 to 0:\n",
    "        if (groupId(t[i]) not in groupOft):\n",
    "           groupOft[ groupId(t[i]) ] = t[0:i]\n",
    "```\n",
    "So, an input transaction $t$ will generate $n\\geq 1$ different transactions (each one with a different slice from t) that will be assigned to $n$ different groups (partitions in our distributed map-reduce algorithm). Observe that in any case $n \\leq m$ (a transaction is sliced at most the number of groups we have)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To understand better how this function makes the different group partitions, consider the following set of six transactions already sorted by the set $L_1=[c,a,m,f,p]$:\n",
    "$$\n",
    "    \\begin{array}{l}\n",
    "      t_1 = \\{ c,a,m,f,p\\} \\\\\n",
    "      t_2 = \\{ c,a,m \\} \\\\\n",
    "      t_3 = \\{ c,a,f \\} \\\\\n",
    "      t_4 = \\{ c,a,m,p \\} \\\\\n",
    "      t_5 = \\{ c,f\\} \\\\\n",
    "      t_6 = \\{ a,m \\}\n",
    "    \\end{array}\n",
    "$$\n",
    "and assume we have divided elements in two groups: group 1: {f,c} and group 2: {a,m,p} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, these are the different group dependent transactions generated for each group:\n",
    "$$\n",
    "    \\begin{array}{l|l|l} \n",
    "      \\textrm{input transaction} & \\textrm{group {f,c}} & \\textrm{group {a,m,p}} \\\\ \\hline\n",
    "      \\{ c,a,m,f,p\\} & \\{c,a,m,f \\}  &  \\{ c,a,m,f,p \\} \\\\\n",
    "      \\{ c,a,m \\}   & \\{ c \\}     &  \\{ c,a,m \\} \\\\\n",
    "      \\{ c,a,f \\}   & \\{ c,a,f \\} &  \\{ c,a \\}        \\\\\n",
    "      \\{ c,a,m,p \\} & \\{ c \\}    & \\{c,a,m,p \\} \\\\\n",
    "      \\{ c,f\\}     &   \\{ c,f\\}  &            \\\\\n",
    "      \\{ a,m \\}    &             & \\{ a,m \\}    \\\\ \\hline\n",
    "    \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two remarks:\n",
    "- group dependent transactions with only one element can be ignored, as counting singleton sets is already done in $L_1$. So, we can consider that the group dependent transactions are:\n",
    "$$ \n",
    "    \\begin{array}{ll} \n",
    "      \\textrm{group(fc)} = & \\{ \\{c,a,m,f \\}, \\{ c,a,f \\}, \\{ c,f\\} \\} \\\\\n",
    "      \\textrm{group(amp)} = & \\{ \\{ c,a,m,f,p \\}, \\{ c,a,m \\}, \\{ c,a \\}, \\{c,a,m,p \\},\\{ a,m \\} \\} \n",
    "    \\end{array}\n",
    "$$ \n",
    "- There is redundant information between both groups of transactions, but the search-tree of a group will be used to count **only patterns** that end with an element of the group. The way of assigning group dependent transactions to groups ensures that the number of transactions that contain a pattern ending with element 'e' will be preserved in the group of transactions associated with 'e' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FP-growth in RDD library\n",
    "\n",
    "Let's see how we use this algorithm in spark when looking for all the frequent subsets of the following sample set of transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beeranddiapers1 = [['beer','diapers','cheese'],\n",
    "                   ['beer','diapers','pizza'],\n",
    "                   ['beer','diapers','pizza','cheese'],\n",
    "                   ['beer','diapers','milk'],\n",
    "                   ['beer','diapers','milk','pizza'],\n",
    "                   ['beer','diapers','toothpaste'],\n",
    "                   ['beer','diapers','icecream'],\n",
    "                   ['beer','diapers','pizza','yogurt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth as rddFPGrowth\n",
    "\n",
    "# make a RDD with a list of lists\n",
    "transactions = sc.parallelize( beeranddiapers1 )\n",
    "model = rddFPGrowth.train(transactions, minSupport=0.25, numPartitions=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['cheese'], freq=2)\n",
      "FreqItemset(items=['cheese', 'diapers'], freq=2)\n",
      "FreqItemset(items=['cheese', 'diapers', 'beer'], freq=2)\n",
      "FreqItemset(items=['cheese', 'beer'], freq=2)\n",
      "FreqItemset(items=['beer'], freq=8)\n",
      "FreqItemset(items=['diapers'], freq=8)\n",
      "FreqItemset(items=['diapers', 'beer'], freq=8)\n",
      "FreqItemset(items=['pizza'], freq=4)\n",
      "FreqItemset(items=['pizza', 'diapers'], freq=4)\n",
      "FreqItemset(items=['pizza', 'diapers', 'beer'], freq=4)\n",
      "FreqItemset(items=['pizza', 'beer'], freq=4)\n",
      "FreqItemset(items=['milk'], freq=2)\n",
      "FreqItemset(items=['milk', 'diapers'], freq=2)\n",
      "FreqItemset(items=['milk', 'diapers', 'beer'], freq=2)\n",
      "FreqItemset(items=['milk', 'beer'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "# Collect back the set of ALL frequent itemsets with minSupport\n",
    "# Beware !, this set could be exponentially large\n",
    "rddfreqitemsets = model.freqItemsets()\n",
    "result = rddfreqitemsets.collect()\n",
    "for fi in result:\n",
    "    print( fi )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, because model.freqItemsets() is a RDD, we can instead save them to a data file, or filter only some of them to be analyzed by our program back in the driver application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent pairs: \n",
      "FreqItemset(items=['cheese', 'diapers'], freq=2)\n",
      "FreqItemset(items=['cheese', 'beer'], freq=2)\n",
      "FreqItemset(items=['diapers', 'beer'], freq=8)\n",
      "FreqItemset(items=['pizza', 'diapers'], freq=4)\n",
      "FreqItemset(items=['pizza', 'beer'], freq=4)\n",
      "FreqItemset(items=['milk', 'diapers'], freq=2)\n",
      "FreqItemset(items=['milk', 'beer'], freq=2)\n",
      "\n",
      "Frequent triples: \n",
      "FreqItemset(items=['cheese', 'diapers', 'beer'], freq=2)\n",
      "FreqItemset(items=['pizza', 'diapers', 'beer'], freq=4)\n",
      "FreqItemset(items=['milk', 'diapers', 'beer'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "pairs = rddfreqitemsets.filter( lambda x  : len(x.items) == 2  ).collect()\n",
    "\n",
    "print ( \"Frequent pairs: \" )\n",
    "for fi in pairs:\n",
    "    print( fi )\n",
    "    \n",
    "print ( \"\\nFrequent triples: \" )\n",
    "triples = rddfreqitemsets.filter( lambda x  : len(x.items) == 3  ).collect()    \n",
    "for fi in triples:\n",
    "    print ( fi )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FP-growth in dataframes library\n",
    "\n",
    "This algorithm is also implemented in the ml data frames library of spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              values|\n",
      "+---+--------------------+\n",
      "|  0|[beer, diapers, c...|\n",
      "|  1|[beer, diapers, p...|\n",
      "|  2|[beer, diapers, p...|\n",
      "|  3|[beer, diapers, m...|\n",
      "|  4|[beer, diapers, m...|\n",
      "|  5|[beer, diapers, t...|\n",
      "|  6|[beer, diapers, i...|\n",
      "|  7|[beer, diapers, p...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth as dfFPGrowth\n",
    "sqlCtx = pyspark.sql.SparkSession(sc)\n",
    "df = sqlCtx.createDataFrame( [ (id,beeranddiapers1[id]) for id in range(8) ], [\"id\",\"values\"] )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|            [cheese]|   2|\n",
      "|   [cheese, diapers]|   2|\n",
      "|[cheese, diapers,...|   2|\n",
      "|      [cheese, beer]|   2|\n",
      "|              [beer]|   8|\n",
      "|           [diapers]|   8|\n",
      "|     [diapers, beer]|   8|\n",
      "|             [pizza]|   4|\n",
      "|    [pizza, diapers]|   4|\n",
      "|[pizza, diapers, ...|   4|\n",
      "|       [pizza, beer]|   4|\n",
      "|              [milk]|   2|\n",
      "|     [milk, diapers]|   2|\n",
      "|[milk, diapers, b...|   2|\n",
      "|        [milk, beer]|   2|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpGrowth = dfFPGrowth(itemsCol=\"values\", minSupport=0.25, numPartitions=4)\n",
    "model = fpGrowth.fit(df)\n",
    "model.freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
