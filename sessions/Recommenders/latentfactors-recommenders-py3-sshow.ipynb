{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <span style=\"color:blue; font-family:Georgia;  font-size:2em;\">\n",
    "        <h1>Recommender systems based on collaborative filtering and latent factors</h1></span>\n",
    " </center>\n",
    "        <p> </p>\n",
    "        <p> </p>\n",
    "        <center><span style=\"color:blue; font-family:Georgia;  font-size:1em;\">\n",
    "        Ramon BÃ©jar Torres</span></center>\n",
    "        <canvas id=\"myCanvas\" width=\"200\" height=\"100\" style=\"border:0px solid\"></canvas>\n",
    "        <center>Data mining - Master on Computer Science</center>\n",
    "        <center><img src=\"M-UdL2.png\"  width=\"200\" alt=\"UdL Logo\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are going to discuss an approach to build a recommender system based on using the whole information stored in the user-product ranking matrix. That is, a matrix where:\n",
    "        $$ M[i,j]  = \\left\\{ \\begin{matrix}  \\textrm{ranking  information  about  user  i  and  product  j} \\\\ Nan \\ \\textrm{if  such ranking is not yet known} \\end{matrix} \\right. $$\n",
    "        \n",
    "We assume that such matrix gives **explicit rankings** provided by users to the products they have bought.\n",
    "\n",
    "Because we use the whole information in such matrix to predict unknown entries, we say that we follow a collaborative (or global) filtering approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The approach based on latent factors, is based on two complementary assumptions:\n",
    "1. User i can be encoded as a vector of c feature values, that encode the preferences of the user for different categories/kinds of products.\n",
    "2. Product j can also be encoded as a vector of c feature values, that encode the weight that the product has in each one of these categories.\n",
    "\n",
    "So, how much an user likes a product will be measured as the **dot product** of these two vectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark-3.0.1-bin-hadoop2.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.109:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Our preliminary set-up code\n",
    "#\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "print (spark_home)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## U-V matrix factorization for latent factors discovery\n",
    "\n",
    "The approach to discover latent factors that characterize users and products, is the one based on U-V matrix factorization. The problem formulation is that we want to find a factorization of our user-product matrix as the product of two matrices $U$ and $V$, where:\n",
    "1. Each user will be represented as a row vector with $c$ factors in matrix $U$, so $U$ will be a matrix with $m$ rows and $c$ columns.\n",
    "2. Each product will be represented as a column vector with $c$ factors in matrix $V$, so $V$ will be a matrix with $c$ rows and $n$ columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, given our user-product rating matrix, with $m$ rows (users) and $n$ columns (products) we want to find matrices $U$ and $V$ such:\n",
    "\n",
    "$$ \\overbrace{ \\left( \\begin{matrix}\n",
    "  U_{1,1} & \\cdots & U_{1,c} \\\\\n",
    "   & \\cdots & \\\\\n",
    "  & \\vdots & \\\\\n",
    "  U_{m,1} & \\cdots & U_{m,c}\n",
    " \\end{matrix} \\right) }^{U \\ (m\\times c)} \\times\n",
    " \\overbrace{ \\left( \\begin{matrix}\n",
    "  V_{1,1} & \\cdots & V_{1,n} \\\\\n",
    "   & \\cdots & \\\\\n",
    "  & \\vdots & \\\\\n",
    "  V_{c,1} & \\cdots & V_{c,n}\n",
    " \\end{matrix}  \\right)  }^{V \\ (c\\times n) } \\ = \\  \n",
    "  \\overbrace{ \\left( \\begin{matrix}\n",
    "  M_{1,1} & \\cdots & M_{1,n} \\\\\n",
    "   & \\cdots & \\\\\n",
    "  & \\vdots & \\\\\n",
    "  M_{m,1} & \\cdots & V_{m,n}\n",
    " \\end{matrix}  \\right)  }^{M (m \\times n)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that in general the matrix $M$ may have empty entries, but our U-V factorization will provide values for all the entries, so when we say an U-V factorization is good for a partially filled matrix M, we usually mean that it agrees with the value for the filled entries of M. Once we have this factorization, observe that the value of any entry $(i,j)$ of the matrix M, even for unknown entries, is predicted multiplying the row $i$ of matrix $U$ by column $j$ of matrix V, that is:\n",
    "\n",
    "$$ \\hat{M}(i,j) = \\sum_{k=1}^c U_{i,k} * V_{k,j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because it may be not possible to find such **exact** factorization with the desired number of latent factors, finding such factorization is actually presented as an optimization problem, where the goal is to find a factorization with the smallest RMSE error (error computed over the filled entries of M):\n",
    "\n",
    " $$ \\textrm{RMSE}(U,V,M) = \\sqrt{\\frac{\\sum_{i,j}(U(row_i) \\ \\cdot \\ V(col_j) - M(i,j))^2}{\\# \\ known \\ entries}} $$\n",
    " where the summatory is over entries $(i,j)$ of the matrix M with known values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, because we are really more interested in being able to **predict unknown values** of the matrix M, than in predicting the known ones, to avoid **overfitting** to the known values and  have a high error rate for unknown entries, the optimization algorithms usually also allow to consider alternative objective functions that incorporate regularized terms to control the tradeoff between:\n",
    "- RMSE error\n",
    "- Prediction of unknown entries.\n",
    "\n",
    "In general, the regularized terms have the goal to penalize too complex models that are very sensitive to small changes in the input. That is, to penalize models where many values of $row_i$ (user i) or $column_j$ (product j) are away from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization algorithms for U-V matrix factorization\n",
    "\n",
    "We may consider different algorithms for finding a good U-V factorization (with a small RMSE error or any other generalized error function). These are the two main ones:\n",
    "1. Gradient descend: Using the same approach we presentend for finding a linear model, but this time applied to the parameters of our model (the coefficients of the matrices U and V)\n",
    "\n",
    "2. Alternating Least Squares: If we keep fixed one of the two matrices (U or V), the problem becomes a quadratic optimization problem that can be optimally solved in P-time. So, we perform an iterative process where we fix one of them, find the optimal one for the other, and in the next iteration we exchange the roles: the second one is fixed and the first one is optimized. This process is repeated until we reach a fixed point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "To know more about this problems and their solving algorithms, a good source of information is the paper:\n",
    "\n",
    "> Yehuda Koren, Robert Bell and Chris Volinsky. *Matrix factorization techniques for recommender systems*. In Computer Journal, IEEE press, Vol 42(8), 2009. \n",
    "https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\n",
    "\n",
    "and also the book about data mining for big data we recommended for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider the same example matrix we used in our notebook about clustering algorithms. In particular, let's consider a data base of users, where for each user we store the ratings given by the user to different movies. We first consider the matrix full of entries, but later we consider variations of this matrix where some of the entries will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "#\n",
    "# We have 10 users, and 10 movies: STW1, STW2, STW3, STW4, STW5, STW6\n",
    "#                                  T1, T2, T3 and BaT\n",
    "# Each entry i,j is the rating given by the user in the range [-5.0,5.0]\n",
    "# We can observe that we have 4 clear Star Wars fans (that they also like a \n",
    "# little bit Terminator movies)\n",
    "# We also have four clear Terminator fans (that they also like a little STWs movies)\n",
    "# Finally, we have two clear Breakfast at tiffannies fans (BaT), that they do not\n",
    "# like too much science-fiction movies\n",
    "\n",
    "usersandmovies = [ [3,3,3,5,5,4, 3,3,-1, -1], \\\n",
    "                   [3,3,3,5,5,4, 4,2,0, -1], \\\n",
    "                   [3,3,4,5,5,4, 4,4,1, 0], \\\n",
    "                   [4,3,3,4,5,4, 3,3,1, -1], \\\n",
    "                   [1,1,1,0,1,1, 5,4,2, -1], \\\n",
    "                   [1,2,1,0,1,1, 4,4,2, -1], \\\n",
    "                   [1,2,2,1,1,1, 4,4,2, -1], \\\n",
    "                   [1,2,2,1,1,0, 5,4,3, -1], \\\n",
    "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4], \\\n",
    "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4]   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "# We need a function to convert our matrix format to a RDD of \n",
    "# pyspark.mllib.recommendation.Rating objects:\n",
    "#\n",
    "#    Rating(int(userid),int(productid),float(rating)\n",
    "# \n",
    "# BEWARE: this function works with the whole matrix in the driver memory,\n",
    "# obtaining a python list representation of the ratings to finally get the\n",
    "# RDD. A better (more scalable) version should do this from a RDD with the matrix entries\n",
    "# loaded from a source file, not from a python matrix in main memory\n",
    "\n",
    "def convertMatrixToRatings( matrix ):\n",
    "    ratings = []\n",
    "    for user,userrow in enumerate(matrix):\n",
    "        for product,productrating in enumerate(userrow):\n",
    "            ratings.append( Rating( int(user) , int(product), float(productrating) ) )\n",
    "    return ratings       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ratings = sc.parallelize( convertMatrixToRatings( usersandmovies ) )\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 3\n",
    "numIterations = 10\n",
    "model = ALS.train(ratings, rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.08856746050130206\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "# First, get the data without the rating values (only user-product IDs)\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Next, Get the predictions obatined with our U-V factorization model\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# join ((u,p), V) and ((u,p), W) to get ((u,p), (V, W))\n",
    "ratesAndPreds = ratings.map( lambda r: ((r[0], r[1]), r[2]) ).join(predictions)\n",
    "\n",
    "## Compute Mean Square error\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Users latent factors:\n",
      "(0, array('d', [-0.8929557204246521, 1.3610984086990356, -0.6500320434570312]))\n",
      "(1, array('d', [-0.7475835680961609, 1.4006414413452148, -0.4298742115497589]))\n",
      "(2, array('d', [-0.8469208478927612, 1.4760514497756958, 0.24045835435390472]))\n",
      "(3, array('d', [-0.34039148688316345, 1.4697706699371338, -0.17931914329528809]))\n",
      "(4, array('d', [0.6612030863761902, 0.7702144980430603, 2.4479691982269287]))\n",
      "(5, array('d', [0.8096885681152344, 0.8089669942855835, 2.1741440296173096]))\n",
      "(6, array('d', [0.6402640342712402, 0.8823555707931519, 2.0161702632904053]))\n",
      "(7, array('d', [0.8778318166732788, 0.9204967617988586, 2.5905468463897705]))\n",
      "(8, array('d', [-1.7340666055679321, -0.9847530126571655, 0.29688316583633423]))\n",
      "(9, array('d', [-1.7340666055679321, -0.9847530126571655, 0.29688316583633423]))\n"
     ]
    }
   ],
   "source": [
    "# We can also get the U-V factorization, as the set of user features (latent factors) of\n",
    "# the U matrix\n",
    "\n",
    "print (\" Users latent factors:\")\n",
    "for userfactors in model.userFeatures().sortByKey().collect():\n",
    "    print (userfactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Products latent factors:\n",
      "(0, array('d', [-0.09388906508684158, 2.150222063064575, -0.3135751783847809]))\n",
      "(1, array('d', [0.39158087968826294, 2.266158103942871, -0.2143813520669937]))\n",
      "(2, array('d', [-0.15328878164291382, 2.194775342941284, -0.08801257610321045]))\n",
      "(3, array('d', [-1.4750709533691406, 2.553581714630127, -0.2178601771593094]))\n",
      "(4, array('d', [-0.667518675327301, 3.071117639541626, -0.45548638701438904]))\n",
      "(5, array('d', [-0.7910909652709961, 2.3469951152801514, -0.2746089994907379]))\n",
      "(6, array('d', [-1.0103960037231445, 2.2092952728271484, 1.427252173423767]))\n",
      "(7, array('d', [-0.8621616959571838, 1.9098454713821411, 1.2768445014953613]))\n",
      "(8, array('d', [0.41049206256866455, 0.4889748692512512, 0.6707598567008972]))\n",
      "(9, array('d', [-1.4981082677841187, -1.190435528755188, 0.5192508697509766]))\n"
     ]
    }
   ],
   "source": [
    "# and the set of product features of the V matrix:\n",
    "print (\"\\n Products latent factors:\")\n",
    "for productfactors in model.productFeatures().sortByKey().collect(): \n",
    "    print (productfactors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you identify significant differences between the latent factors for different user groups and product groups ? Observe that the two \"Breakfast at Tiffany's\" fans have clearly different latent factors than the others users\n",
    "\n",
    "Let's next check what happens if there are some missing values in the user-product matrix. Consider the following function to randomly erase $k$ entries from each row of the matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction model with incomplete data\n",
    "\n",
    "Let's consider now the extraction of some of the elements of the user-product matrix. Consider randomly eliminating k elements from each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def convertMatrixToRatingsWithBlanks( matrix, k ):\n",
    "    ratings = []\n",
    "    for user,userrow in enumerate(matrix):\n",
    "        size = len(userrow)\n",
    "        blanks = random.sample(range(size), k)\n",
    "        for product,productrating in enumerate(userrow):\n",
    "            if (product not in blanks):\n",
    "              ratings.append( Rating( int(user) , int(product), float(productrating) ) )\n",
    "    return ratings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ratings2 = sc.parallelize( convertMatrixToRatingsWithBlanks( usersandmovies, 4  ) )\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 3\n",
    "numIterations = 10\n",
    "model2 = ALS.train(ratings2, rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.011390722198382629 Mean Square Error over complete data: 0.845641482048498\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "# First, get the data without the rating values (only user-product IDs)\n",
    "testdata2 = ratings2.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Next, Get the predictions obatined with our U-V factorization model\n",
    "predictions2 = model2.predictAll(testdata2).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# join ((u,p), V) and ((u,p), W) to get ((u,p), (V, W))\n",
    "ratesAndPreds2 = ratings2.map( lambda r: ((r[0], r[1]), r[2]) ).join(predictions2)\n",
    "\n",
    "# Make predictions also for the whole dataset\n",
    "predictAll = model2.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPredsAll = ratings.map( lambda r: ((r[0], r[1]), r[2]) ).join(predictAll)\n",
    "\n",
    "## Compute Mean Square error\n",
    "MSE2 = ratesAndPreds2.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "MSEAll = ratesAndPredsAll.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "\n",
    "print(\"Mean Squared Error = \" + str(MSE2) +  \" Mean Square Error over complete data: \"+str(MSEAll) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Results for different values for k (eliminated entries from each row):\n",
    "\n",
    "$$ \n",
    "\\begin{matrix}\n",
    " k & MSE & MSE \\ \\textrm{for all data} \\\\ \\hline\n",
    "1 & 0.075 & 0.10 \\\\\n",
    "2 & 0.066 & 0.17 \\\\ \n",
    "3 & 0.037 & 0.33 \\\\\n",
    "4 & 0.011 & 0.84  \n",
    "\\end{matrix} \n",
    "$$\n",
    "\n",
    "But different executions with the same k will give you slightly different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, the error can decrease when there are less entries available to fit a good model, but this does not imply that the model predition on unkown entries will be also better. Actually, a problem of this factorization approach is that to be able to get a good factor vector for a given user, we need enough known entries for that user.\n",
    "\n",
    "In contrast, there are recommender systems for products in on-line stores based on direct product-to-product comparison that allow to give recommendations for users from any single previously bought (or ranked) product of that user. So, even if the user only made ONE purchase the system will be able to give recommendations, as soon as the total number of user purchases in the on-line store is high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collaborative Filtering for Implicit Feedback Datasets\n",
    "\n",
    "We can also consider data where there is no explicit user feedback. That is, when the entry $M(i,j)$ does not contain an explicit rating of the user, but some **aggregation** of values that can be used to infer something about the preference of user i for product j\n",
    "\n",
    "For example:\n",
    "- In a web shop, $M(i,j)$ could indicate the number of times that the user i clicked on product j\n",
    "- In a on-line video shop, $M(i,j)$ could indicate the fraction of watched time for movie $j$ by user $i$\n",
    "- We could also give more relevance if the movie was watched with less interruptions.\n",
    "\n",
    "In general, we can set up $M(i,j)$ to represent some aggregation of indicators such that the more inputs we have, the higher the value of $M(i,j)$, and so the higher our confidence with respect to the preference of user $i$ for product $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To know more about the ALS algorithm (and other global filtering algorithms) in spark, check the documentation page:\n",
    "\n",
    "> https://spark.apache.org/docs/3.0.0/ml-collaborative-filtering.html\n",
    "\n",
    "To know more about the model used when we assume that the matrix contains implicit feedback data, you can check here one possible model:\n",
    "\n",
    "> \"Collaborative Filtering for Implicit Feedback Datasets\". http://yifanhu.net/PUB/cf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
