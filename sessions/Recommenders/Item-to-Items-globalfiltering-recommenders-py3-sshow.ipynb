{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <span style=\"color:blue; font-family:Georgia;  font-size:2em;\">\n",
    "        <h1>Recommender systems based on item-to-item  collaborative filtering</h1></span>\n",
    " </center>\n",
    "        <p> </p>\n",
    "        <p> </p>\n",
    "        <center><span style=\"color:blue; font-family:Georgia;  font-size:1em;\">\n",
    "        Ramon BÃ©jar Torres</span></center>\n",
    "        <canvas id=\"myCanvas\" width=\"200\" height=\"100\" style=\"border:0px solid\"></canvas>\n",
    "        <center>Data mining - Master on Computer Science</center>\n",
    "        <center><img src=\"M-UdL2.png\"  width=\"200\" alt=\"UdL Logo\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this notebook we consider how to build recommender systems based on item-to-item (where items are the products of the on-line store) collaborative filtering.\n",
    "\n",
    "The approach to recommend new items to an user is based on finding SIMILAR items to the ones already bought by the user, or similar to the ones we know the user likes or is interested in. So, what we use in this kind of recommender systems is a way to compare two items, but based on the information from the set of users that bought those products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This approach is a good one when the user-item matrix represents which products were bought by each user, but the typical user row vector is sparse (there are few purchases with respect to the total number of items of the on-line store).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This item-centered approach allows us to compute recommendations for any single item bought by any user.\n",
    "\n",
    "Even in the extreme case where the user has only bought one item, the system will be able to compute good recommendations if the total number of purchases of the on-line store is high enough.\n",
    "\n",
    "We can also consider not only which items the user bought, but also wich items he has browsed in the online store frequently, or any other source of implicit knowledge about the interests of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In any case, the main issue here will be **how to compare items of the on-line store** and how to predict wich ones will be more interesting for the user.\n",
    "\n",
    "So, with respect to the approach based on latent factors:\n",
    "- Here we also perform global filtering because we consider the whole set of users and items to build the system\n",
    "- But we compute explicitely similarity measures between any pair of products, instead of decomposing users and products as vectors of latent factors to predict the matching between users and products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Preliminary start-up code for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark-3.0.0-bin-hadoop2.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.122.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "# from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print (spark_home)\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing items based on set of common costumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first step towards discovering (mining) similar items is how to measure the similarity between two items in the store catalog.\n",
    "\n",
    "If we assume that the information we have for every item is the set of users that bought that item, the global filtering approach is based on comparing ALL the costumers of the store, checking how many costumers have bought both the two items we want to compare. \n",
    "\n",
    "> The idea is that the similarity measure should be higher when the two items have a higher number of common costumers.\n",
    "\n",
    "So, a very direct way of measuring the distance would be to count the number of common costumers, and normalize it by the total number of costumers of the store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The cosine distance\n",
    "\n",
    "However, when we consider user-item matrices that incorporate more complex information about user-item pairs, for example the satisfaction degree of the costumer with the item, it is better to consider other kinds of similarity measures. One widely used distance measure is the cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given the angle $\\theta$ between two product vectors v1 and v2, the cosine distance is defined as the cosine of the angle $\\theta$, that we can compute from the components of both vectors as follows:\n",
    "\n",
    "$$ cosdis(v_1,v_2) = cos(\\theta) = \\frac{v_1 \\cdot v_2}{\\sqrt{\\sum_i v_1[i]^2}\\sqrt{\\sum_i v_2[i]^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some key properties:\n",
    "- The value ranges in [-1,1]\n",
    "- -1 means they are opposite vectors ($\\theta=180$)\n",
    "- 0 means they are orhogonal vectors ($\\theta=90$)\n",
    "- 1 means they are equal or proportional (same direction) ($\\theta=0$)\n",
    "\n",
    "We can compute this measure only when both vectors are non-zero (the module of any of them is not zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, when used as a similarity measure, observe the cosine distance can signal from totally different products (-1) to totally similar ones (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For working on item-to-item based collaborative filtering, we must work with the columns of the user-item matrix, the same matrix that we used with global filtering based on finding latent factors.\n",
    "\n",
    "From now on, we will assume that items are already given as vectors, representing their corresponding column vectors in the user-item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#  Compute the cosine distance between vectors vec1 and vec2, represented\n",
    "#  as dense lists: with all the elements (non-zero and zero) values present\n",
    "def CosineDistance( vec1, vec2 ):\n",
    "    dot = 0.0\n",
    "    v1rs = 0.0\n",
    "    v2rs = 0.0\n",
    "    for i in range(len(vec1)):\n",
    "        dot += (vec1[i]*vec2[i]) \n",
    "        v1rs += (vec1[i]*vec1[i])\n",
    "        v2rs += (vec2[i]*vec2[i])\n",
    "    v1rs = math.sqrt(v1rs)\n",
    "    v2rs = math.sqrt(v2rs)\n",
    "    return dot/(v1rs*v2rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, working with sparse vectors is a better approach if we consider that the fraction of users that have bought a product will be low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Binary feature vectors\n",
    "\n",
    "Consider for example the following set of 4 item vectors, each one indicating which costumers, from a total set of 4 costumers, have bought (1) or have not bought (0) the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "itemvectors = [ [1,1,0,0], [0,1,0,1], [0,1,0,0], [1,0,1,0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 1 : 0.4999999999999999\n",
      "Cosine distance between  0 and 2 : 0.7071067811865475\n",
      "Cosine distance between  0 and 3 : 0.4999999999999999\n",
      "Cosine distance between  1 and 2 : 0.7071067811865475\n",
      "Cosine distance between  1 and 3 : 0.0\n",
      "Cosine distance between  2 and 3 : 0.0\n"
     ]
    }
   ],
   "source": [
    "for i1, vec1 in enumerate(itemvectors):\n",
    "    for i2, vec2 in enumerate(itemvectors):\n",
    "        if (i1 < i2):\n",
    "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Negative and positive features vectors\n",
    "\n",
    "As we have said, the cosine distance is also defined when our item vectors contain values in a range that includes negative values. This is the case where:\n",
    "* negative values mean *negative* ratings\n",
    "* positive values mean positive ratings\n",
    "* 0 mean a neutral rating (or no rating at all). For example, consider the following set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "itemvectors2 = [ [-3,-3,-3,-2], [-2,-2,-2,-1], [1,1,1,1], [3,3,3,3] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 1 : 0.996270962773436\n",
      "Cosine distance between  0 and 2 : -0.987829161147262\n",
      "Cosine distance between  0 and 3 : -0.987829161147262\n",
      "Cosine distance between  1 and 2 : -0.9707253433941511\n",
      "Cosine distance between  1 and 3 : -0.9707253433941511\n",
      "Cosine distance between  2 and 3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "for i1, vec1 in enumerate(itemvectors2):\n",
    "    for i2, vec2 in enumerate(itemvectors2):\n",
    "        if (i1 < i2):\n",
    "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe that in this case, the cosine distance ranges from -1 to 1, where -1 means *totally opposite vectors*, and 1 means totally aligned vectors, without giving relevance to the magnitude of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Similarity in the users-movies example\n",
    "\n",
    "Let's consider what information the cosine distance provides when comparing movies in our users-movies example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We have 10 users, and 10 movies: STW1, STW2, STW3, STW4, STW5, STW6\n",
    "#                                  T1, T2, T3 and BaT\n",
    "usersandmovies = [ [3,3,3,5,5,4, 3,3,-1, -1], \\\n",
    "                   [3,3,3,5,5,4, 4,2,0, -1], \\\n",
    "                   [3,3,4,5,5,4, 4,4,1, 0], \\\n",
    "                   [4,3,3,4,5,4, 3,3,1, -1], \\\n",
    "                   [1,1,1,0,1,1, 5,4,2, -1], \\\n",
    "                   [1,2,1,0,1,1, 4,4,2, -1], \\\n",
    "                   [1,2,2,1,1,1, 4,4,2, -1], \\\n",
    "                   [1,2,2,1,1,0, 5,4,3, -1], \\\n",
    "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4], \\\n",
    "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4]   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def getMovieVector( usersandmovies, j ):\n",
    "    return [ usersandmovies[u][j] for u in range(len(usersandmovies))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarity between the first four star wars movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 1 : 0.9554528219538876\n",
      "Cosine distance between  0 and 2 : 0.9668114046189884\n",
      "Cosine distance between  0 and 3 : 0.8808819893600309\n",
      "Cosine distance between  1 and 2 : 0.9698160576680458\n",
      "Cosine distance between  1 and 3 : 0.772771255454225\n",
      "Cosine distance between  2 and 3 : 0.8762691935871222\n"
     ]
    }
   ],
   "source": [
    "swmovies = [ getMovieVector( usersandmovies, j ) for j in range(4)]\n",
    "for i1, vec1 in enumerate(swmovies):\n",
    "    for i2, vec2 in enumerate(swmovies):\n",
    "        if (i1 < i2):\n",
    "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarity between terminator movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 1 : 0.9824666109905364\n",
      "Cosine distance between  0 and 2 : 0.7681373347487839\n",
      "Cosine distance between  1 and 2 : 0.7767356373806175\n"
     ]
    }
   ],
   "source": [
    "tmovies = [ getMovieVector( usersandmovies, j ) for j in [6,7,8]]\n",
    "for i1, vec1 in enumerate(tmovies):\n",
    "    for i2, vec2 in enumerate(tmovies):\n",
    "        if (i1 < i2):\n",
    "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarity between the first four star wars movies and terminator movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between  0 and 0 : 0.7393877297305068\n",
      "Cosine distance between  0 and 1 : 0.73431307102251\n",
      "Cosine distance between  0 and 2 : 0.4495530025457533\n",
      "Cosine distance between  1 and 0 : 0.7762444233578819\n",
      "Cosine distance between  1 and 1 : 0.7741809609998153\n",
      "Cosine distance between  1 and 2 : 0.5989849814784503\n",
      "Cosine distance between  2 and 0 : 0.8135251376128875\n",
      "Cosine distance between  2 and 1 : 0.8113625732861212\n",
      "Cosine distance between  2 and 2 : 0.552422157047008\n",
      "Cosine distance between  3 and 0 : 0.6859384573602509\n",
      "Cosine distance between  3 and 1 : 0.6673778622698389\n",
      "Cosine distance between  3 and 2 : 0.18302666282596894\n"
     ]
    }
   ],
   "source": [
    "for i1,vec1 in enumerate(swmovies):\n",
    "    for i2,vec2 in enumerate(tmovies):\n",
    "        print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So observe that many star wars movies can be considered to be *similar* to many terminator movies (from the point of view of our set of users). But not all such pairs are clearly similar. For example:\n",
    "- Similarity for (SW1,T3) : 0.44\n",
    "- Similarity for (SW4,T3) : 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarity between Star Wars I and Breakfast at Tiffanys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between 0 and 1 : -0.6477502756312957\n"
     ]
    }
   ],
   "source": [
    "vec1, vec2 = getMovieVector( usersandmovies, 0 ), getMovieVector( usersandmovies, 9 )\n",
    "print (\"Cosine distance between 0 and 1 :\", CosineDistance( vec1, vec2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, in this case we get a negative value, because it seems that Star Wars and Breakfast at Tiffanys are very opposite movies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mining similar items based on item-to-item global filtering\n",
    "\n",
    "Let's now consider the approach for recommending products to the Amazon costumers presented in the paper:\n",
    "\n",
    "> Greg Linden, Brent Smith, and Jeremy York. *Amazon.com recommendations: Item-to-Item Collaborative Filtering*.\n",
    "> In IEEE INTERNET COMPUTING. 2003\n",
    "\n",
    "In that paper you will find only the overall idea. The approach is the one we have explained before, use some similarity measure between products to recommend relevant products. We do not know what are the current similarity measures used by Amazon, but we will use the cosine distance in this notebook to develop our recommender system.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main building block of the Amazon recomender system is their algorithm to compute similarity between any pair of items in their on-line catalog. This is the pseudo-code of the mentioned Amazon item-to-item similarity mining algorithm:\n",
    "\n",
    "```python\n",
    "def computeSimilarityBetweenProducts( I ):\n",
    " for each item i1 in product catalog I:\n",
    "    for each customer C who purchased i1:\n",
    "       for each item i2 purchased by customer C:\n",
    "          record that *a customer* (C) purchased i1 and i2:\n",
    "             store that (i1,i2) were purchased by a same user\n",
    "    for each item i2 in product catalog I:\n",
    "        compute the similarity between i1 and i2\n",
    "```\n",
    "\n",
    "This algorithm can be though as an **off-line** algorithm, the similarity between products should be computed as a background process, and only be recomputed when there are a significant number of changes in the purchases database. What is the worst-case and real complexity of this algorithm ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the following observations (where $M$ is the number of costumers and $N$ the number of items of the on-line store:\n",
    "\n",
    "1. The worst-case complexity is $ O(N^{2}  M)$. This is the case when almost any user has bought any item of the store.  \n",
    "2. However, if we assume that many costumers have very few purchases (let's say a constant number), the real complexity is more closer to $O(N \\ M)$.\n",
    "3. The complexity can be further reduced if we only consider a sample (subset) of costumers to compute the similarity between products. Of course, this will produce an approximation of the real similarity values between products. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For computing the similarity, we can use the cosine distance, or any other similarity measure we think is good for our application domain. Observe that in some sense, it is reasonable to think that:\n",
    "\n",
    "> the similarity between a pair of products (i1,i2) will be *a number* proportional to the total number of customers that purchased both i1 and i2,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distributed computation\n",
    "\n",
    "If we only record whether an user buys a product (1/0 binary feature vectors), then observe that the similarity measure depends only on the total number of such customers, and not the  particular ones.\n",
    "\n",
    "So, the computation needed to compute the similarity between i1 and i2 can be thought as some kind of *reduce* (by Key) operation between all pairs (i1,i2) produced by different users C. That is, if we consider (i1,i2) as the key, and for example \"1\" as the value for each different user C that bought i1 and i2, the (key,value) to produce would be:\n",
    "\n",
    "          ((i1,i2), 1)  for each user C that bought i1 and i2\n",
    "\n",
    "and then reduce by key (for example summing up the values) all such (key,value) pairs. Of course, to get a normalized similarity measure the sum should be divided by the maximun number of users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output format of the data set computed by such global filtering algorithm could be something like the following. For each item I we have a list of (item,similarity) pairs:\n",
    "  \n",
    " I :  [ (j1,sim(I,j1), (j2,sim(I,j2), ..., (jI,sim(I,jI) ]\n",
    " \n",
    " where the set of items j1, j2, ... , jI is the set of items that have at least one common costumer (user) with I and so their similarity is > 0. We will refer to the (distributed) data set that contains such information for all the items as the **rddSimilarityPairs** in the rest of this notebook.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we instead consider the distributed computation of the cosine distance, in the more general case with negative and positive feature values, the computation could be as follows:\n",
    "1. Map every pair of user-product ratings **with the same user u** to the values they contribute in the final cosine distance between p1 and p2:\n",
    "$$  (u,p_1,r_1),(u,p_2,r_2) \\rightarrow ((p_1,p_2),(r_1 r_2,r_1^2,r_2^2) ) $$\n",
    "2. Reduce all the previous key-value pairs, with the same key as:\n",
    "$$ ((p_1,p_2),(pra_{1,2},ra_1^2,ra_2^2) ) + ((p_1,p_2),(prb_{1,2},rb_1^2,rb_2^2) ) \n",
    "   \\rightarrow \\\\  ((p_1,p_2),( pra_{1,2}+prb_{1,2}, ra_1^2+rb_1^2,\n",
    "   ra_2^2+rb_2^2) ) $$\n",
    "3. Compute the cosine distance combining the reduced values in a final map:\n",
    "$$ ((p_1,p_2),(\\sum_u r_1 r_2,\\sum_u r_1^2,\\sum_u r_2^2) ) \\rightarrow \n",
    "\\frac{\\sum_u r_1 r_2}{\\sqrt{\\sum_u r_1^2} \\sqrt{\\sum_u r_2^2}}  $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observe that the previous solution is not necessarily the most efficient way to compute the cosine distance:\n",
    "\n",
    "* It produces redundant information: info for $(p_1,p_2)$ will be the same one as the info for $(p_2,p_1)$. This could be fixed filtering the pairs we combine in step 1 to those with $p_1 < p_2$.\n",
    "\n",
    "* The sum of squares $ \\sum_u r_i^2$ for item $i$ is computed as many times as different pairs of items we build with $i$. So, to save some computation, the term $ \\sum_u r_i^2$ could be instead computed only once, and then reused when computing the cosine distance between item $i$ and any other item $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommending similar items to a previously bought one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a first recommender system, consider the case where we want to recommend similar items to one just bought by an user, or to one recently browsed by the user. So, we want to focus on similar items to a particular one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here you have a possible pseudo-code for a recommender system for that particular case. The input is the user we are considering and the item we want to use as the base item to get the recommendations. Observe that the primary use of this algorithm would be \"on-line\": every time an user makes a new purchase or browses a new item it would be desirable to get such focused recommendations. \n",
    "\n",
    "```python \n",
    "def RecommendKmostSimilar( RDD rddSimilarityPairs, user U, item I, int K ):\n",
    "     rdd1 = getSimilarItems( rddSimilarityPairs, I )\n",
    "     rdd2 = EraseItemsAlreadyBought( rdd1, U )\n",
    "     rdd3 = rdd2.sortBySimilarity()\n",
    "     bestK = rdd3.takeFirstKItems( K )\n",
    "     \n",
    "     return bestK\n",
    "```\n",
    "\n",
    "This function assumes that we have a previously computed data set, the rddSimilarityPairs, that should be the one computed by the similarity mining algorithm of  the previous section (or by any other algorithm that provides such data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation\n",
    "\n",
    "Let's see a possible implementation in spark of the four steps of the previous algorithm. Let's first consider the following similarity information data set, that for simplicity we will consider that is stored in a plain ASCII file. In a final application this similarity information would be stored in a database. \n",
    "\n",
    "Observe that if we assume that the similarity between products does not change significantly very frequently, we can compute the similarity in a off-line algorithm, and only run on-line with an RDD that contains only the similarity between the current target product I and the other products with similarity greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1    2,0.6  4,0.3\n",
    "\n",
    "2    1,0.6\n",
    "\n",
    "3    4,0.7  5,0.8 \n",
    "\n",
    "4    3,0.7  5,0.4 1,0.3\n",
    "\n",
    "5    3,0.8  4,0.4\n",
    "\n",
    "6    7,0.3\n",
    "\n",
    "7    6,0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Format of line:  ItemI    ItemI1,Sim_(I,I1) ...  ItemIN,Sim_(I,IN)\n",
    "# \n",
    "# We assume that the line contains only the information for items such that their similarity with I is > 0\n",
    "#\n",
    "def parseSimilarityInfo( line ):\n",
    "   toks = line.split()\n",
    "   sourceitem = int(toks[0])\n",
    "   targetitems = [ tuple(it.split(',')) for it in toks[1:] ]\n",
    "   targetitems = [ (int(it[0]),float(it[1])) for it in targetitems ]\n",
    "   return (sourceitem,targetitems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 0. Load similarity info from file to RDD  \n",
    "\n",
    "Before computing recommendations, we load the file similarityPairsInfo_1.txt to get the desired RDD\n",
    "data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rddSimilarityPairs = sc.textFile('similarityPairsInfo_1.txt').map( parseSimilarityInfo )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look to check if the file was well parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [(2, 0.6), (4, 0.3)]),\n",
       " (2, [(1, 0.6)]),\n",
       " (3, [(4, 0.7), (5, 0.8)]),\n",
       " (4, [(3, 0.7), (5, 0.4), (1, 0.3)]),\n",
       " (5, [(3, 0.8), (4, 0.4)]),\n",
       " (6, [(7, 0.3)]),\n",
       " (7, [(6, 0.3)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddSimilarityPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe that this format for storing similarity info is efficient towards finding the complete info for an item, but has some redundancy in the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Filter similarity info related to item I\n",
    "\n",
    "Next, we can filter from such rdd data set only the similarity information related to our input item I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def getSimilarItems( rddSimilarityPairs, I ):\n",
    "   return  rddSimilarityPairs.filter( lambda x : x[0] == I )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's test the function with item 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, [(3, 0.7), (5, 0.4), (1, 0.3)])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = getSimilarItems( rddSimilarityPairs, 4 )\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Filter elements not already bought by user U\n",
    "\n",
    "The next step is to retain only items not bought by user U. Again, the implementation is highly dependent on whether we assume the set of purchases of user U fits into a single machine or it must be distributed. Let's assume here that his/her ser of purchases fits into one machine, so we can read it into the following function. We assume this format:\n",
    "\n",
    "   user1  item11   item12 ...  item1N\n",
    "   \n",
    "   user2  item11   item12 ...  item1N\n",
    "   \n",
    "    ...\n",
    "    \n",
    "   userN  item11   item12 ...  item1N\n",
    "   \n",
    "where the set of items in line i is the set of items bought by user i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# In this function we remove the set of already items by U, and also\n",
    "# remove the item key to get the final set\n",
    "# similar and filtered items as a single list\n",
    "def RemoveBoughtItems( itemsSimilarToI, U ):\n",
    "   purchases = getPurchases( U )\n",
    "   print ( \" Purchases by user \", U, \" : \", purchases )\n",
    "   return itemsSimilarToI.flatMap( lambda x : [it for it  in x[1] if it[0] not in purchases ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will consider the next example file of user purchases ('purchases.txt'):\n",
    "\n",
    "    1    2  3\n",
    "    2    3  4\n",
    "    3    4  5  6\n",
    "    4    6  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def getPurchases( U ):\n",
    "    purchases = []\n",
    "    f = open( 'purchases.txt' )\n",
    "    for line in f:\n",
    "        toks = line.split()\n",
    "        if (int(toks[0]) == U):\n",
    "            purchases = toks[1:]\n",
    "            break\n",
    "    f.close()\n",
    "    return [ int(p) for p in purchases ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's check it filtering out the purchases of user 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Purchases by user  3  :  [4, 5, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 0.7), (1, 0.3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = RemoveBoughtItems( rdd1, 3 )\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Sort them and take the most K similar items\n",
    "\n",
    "The last two steps are sort the resulting set of items by similarity and taking the first k items. We can do this with spark with a single action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.7)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only the first most similar non-bought item\n",
    "rdd2.takeOrdered(1, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The meaning of empty entries\n",
    "\n",
    "Observe that in the cosine distance formula:\n",
    "$$ cosdis(v_1,v_2) = cos(\\theta) = \\frac{v_1 \\cdot v_2}{\\sqrt{\\sum_i v_1[i]^2}\\sqrt{\\sum_i v_2[i]^2}} $$\n",
    "\n",
    "the contribution of user i will be 0 if the entries $ v_1[i] $ and $ v_2[i]$ are equal to 0. That is, it would be like simplifying the vectors eliminating the entry $i$. \n",
    "\n",
    "But then, in case the values represent ratings like in the movies example, what should be the contribution of that entry if the value is not 0, but empty (user i did not give ratings for movies $v_1$ and $v_2$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Assuming 0 means empty rating\n",
    "\n",
    "For example, assume the two modified vectors for star wars 1 and star wars 4, where the 0 represents NO rating (instead of neutral ratings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between sw1  [3, 3, 3, 4, 0, 0, 1, 1, -2, -2] and sw4  [5, 5, 5, 4, 0, 0, 1, 1, 0, 0] : 0.8973484983270362\n"
     ]
    }
   ],
   "source": [
    "sw1 = getMovieVector( usersandmovies, 0 )\n",
    "sw4 = getMovieVector( usersandmovies, 3 )\n",
    "sw1[4] = sw1[5] = 0\n",
    "print (\"Cosine distance between sw1 \", sw1, \"and sw4 \", sw4, \":\", CosineDistance( sw1, sw4 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we increase the empty entries that coincide for a same user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between sw1  [3, 3, 3, 4, 0, 0, 1, 1, 0, 0] and sw4  [5, 5, 5, 4, 0, 0, 1, 1, 0, 0] : 0.9738516810963532\n"
     ]
    }
   ],
   "source": [
    "sw1[8] = sw1[9] = 0\n",
    "print (\"Cosine distance between sw1 \", sw1, \"and sw4 \", sw4, \":\", CosineDistance( sw1, sw4 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, having less ratings can induce a higher value for the cosine distance, when the non-empty entries are very similar, even if they are very few with respect to the total number of users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommending based on the whole set of previous purchases\n",
    "\n",
    "Sometimes Amazon also sends emails to costumers sending recommendations based on *all* (or a subset of) his/her previous purchases, instead of based on a single recent one.\n",
    "\n",
    "Observe that in this case, a same item could be recommended but with different similarity values, as the result of being similar to different purchases of the given user (but with a different similarity value with each purchase).\n",
    "\n",
    "That is, to have for a particular item i, that we consider it for recommendation, a set of different similarity values:\n",
    "$$ (i,p_1,s_1), (i,p_2,s_2), \\ldots, (i,p_m,s_m)  $$\n",
    "if item i was found similar to different previous purchases $p_1,p_2,\\ldots,p_m$ of the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, we should **aggregate** in some way the different similarity values\n",
    "$$  s_1, s_2, \\ldots, s_m  $$\n",
    "obtained for a same item. How can we aggregate the different similarities between a given product and a set of different products ?\n",
    "\n",
    "- We could use the average/median value of the set similarity values for a same item\n",
    "- We could use some kind of weighted sum, that gives more relevance to items where the similarity info was computed using the info of more users\n",
    "$$ wSum =  \\sum_i w_i s_i  $$\n",
    "where $w_i$ represents the weight (relevance) of the similarity value $s_i$ towards computing the final value. Observe that setting $w_i=1/m$ gives the regular average value, that gives all the values the same relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring similarity in other ways\n",
    "\n",
    "As we have seen, the key component of this kind of recommender systems is the way to compute similarity between items. We have presented a very common one, the cosine distance, but there are some others, such as the Jaccard distance or the Pearson correlation coefficient.\n",
    "\n",
    "Here you have a good source of information about different ways to measure similarity for recommender systems:\n",
    "\n",
    "> \"A new user similarity model to improve the accuracy of collaborative filtering\" by Haifeng Liu, Zheng Hu, Ahmad Mian, Hui Tian, Xuzhen Zhu. https://doi.org/10.1016/j.knosys.2013.11.006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
